{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b423310c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "from PIL import Image\n",
    "import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "19805cc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Replace with your actual Hugging Face API token\n",
    "HF_TOKEN = os.environ.get(\"HF_TOKEN\") # It's best to use environment variables for tokens\n",
    "\n",
    "# The model ID for Magma\n",
    "# Note: Magma is a multimodal model. Its API might expect specific input formats\n",
    "# (e.g., image + text, or a specific prompt structure).\n",
    "# You'll need to refer to the specific model card on Hugging Face Hub for exact input/output.\n",
    "MODEL_ID = \"microsoft/Magma-8B\"\n",
    "API_URL = f\"https://api-inference.huggingface.co/models/{MODEL_ID}\"\n",
    "\n",
    "headers = {\n",
    "    \"Authorization\": f\"Bearer {HF_TOKEN}\",\n",
    "    \"Content-Type\": \"application/json\" # For text inputs\n",
    "    # For image inputs, you might need \"Content-Type\": \"image/jpeg\" or similar\n",
    "}\n",
    "\n",
    "def query_magma_text(text_input):\n",
    "    payload = {\n",
    "        \"inputs\": text_input\n",
    "        # Magma often expects a specific prompt format, e.g., for VQA or UI navigation\n",
    "        # You might need to structure 'inputs' as:\n",
    "        # \"inputs\": {\"text\": \"<image_start><image><image_end>\\nWhat is in this image?\"}\n",
    "        # or for UI navigation, it might involve image data directly.\n",
    "    }\n",
    "    response = requests.post(API_URL, headers=headers, json=payload)\n",
    "    response.raise_for_status() # Raise an exception for bad status codes\n",
    "    return response.json()\n",
    "\n",
    "def query_magma_image_and_text(image_path, text_input):\n",
    "    # For multimodal models like Magma, you often send binary image data\n",
    "    # along with text. The exact format depends on the model's API.\n",
    "    # This is a simplified example; actual Magma API might be more complex.\n",
    "\n",
    "    # If Magma's API expects image and text in separate parts of the request,\n",
    "    # you might need to use `files` parameter in requests.post or a different structure.\n",
    "    # Check the Magma model card on Hugging Face Hub for its specific Inference API usage.\n",
    "\n",
    "    with open(image_path, \"rb\") as f:\n",
    "        image_data = f.read()\n",
    "\n",
    "    # Some models use a multipart/form-data for image and text\n",
    "    # Or you might send image as base64 in JSON, but binary is common for HF Inference API\n",
    "    # The exact structure depends on the model's inference endpoint.\n",
    "    # For Magma specifically, you might need to combine them within the 'inputs' structure,\n",
    "    # potentially by embedding the image as a string or a special token.\n",
    "\n",
    "    # A common pattern for multimodal inference API:\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {HF_TOKEN}\",\n",
    "        # \"Content-Type\": \"application/json\" if image is base64 encoded within JSON\n",
    "        # Or \"Content-Type\": \"multipart/form-data\" if sending as files\n",
    "    }\n",
    "\n",
    "    # This is an *example* and might not be exactly how Magma's Inference API works for image+text\n",
    "    # You would need to consult the specific API documentation for Microsoft Magma-8B\n",
    "    # on Hugging Face Hub.\n",
    "    # A common pattern involves sending the image bytes directly and the text in headers or a separate field.\n",
    "    # Example using the `huggingface_hub` InferenceClient:\n",
    "    from huggingface_hub import InferenceClient\n",
    "\n",
    "    client = InferenceClient(token=HF_TOKEN)\n",
    "    # Assuming a structure like this for Magma, based on its common usage patterns\n",
    "    # This is a general example for multimodal. Magma might have specific processor calls.\n",
    "    # The prompt format for Magma often involves <image_start><image><image_end> tokens.\n",
    "    # You'd send the image bytes and the text prompt where the image token is placeholder.\n",
    "    # The Inference API would then combine them.\n",
    "\n",
    "    # This specific usage for Magma might involve a custom handler on the endpoint.\n",
    "    # A common simple pattern for some HF image + text models:\n",
    "    # response = requests.post(API_URL, headers=headers, data=image_data) # Send image first\n",
    "    # You might also send JSON for text data in a separate request or merged.\n",
    "\n",
    "    # For Magma, the Hugging Face Space shows a Gradio demo:\n",
    "    # https://huggingface.co/spaces/microsoft/Magma-UI\n",
    "    # This suggests a more integrated approach, typically managed by `transformers` or a custom handler\n",
    "    # on the Inference Endpoint, not just raw text/image over generic API.\n",
    "    # If you want to use the *Inference API* with Magma, you'd need to confirm what inputs it expects.\n",
    "    # Many multimodal models on HF expose an API where you send the image as binary data\n",
    "    # and the text as a JSON payload, or combine them with a multipart request.\n",
    "\n",
    "    # Let's assume for simplicity a model that takes image bytes and text in a JSON:\n",
    "    # This is speculative for Magma's direct Inference API.\n",
    "    # You'll likely need to check the exact endpoint behavior.\n",
    "    # A safer bet is to use the `huggingface_hub` library's `InferenceClient`.\n",
    "    try:\n",
    "        # This is a generic example for a visual question answering task.\n",
    "        # Magma's specific API might require a different client method or payload.\n",
    "        # Refer to https://huggingface.co/docs/huggingface_hub/package_reference/inference_client#huggingface_hub.InferenceClient.image_to_text\n",
    "        # and the Magma model card on Hugging Face for specifics.\n",
    "        result = client.image_to_text(image=image_path, prompt=text_input, model=MODEL_ID)\n",
    "        return result\n",
    "    except Exception as e:\n",
    "        print(f\"Error calling Inference API for multimodal Magma: {e}\")\n",
    "        # The Magma model often works with a specific processor and model loading\n",
    "        # in a Python environment, rather than a generic text_to_image or image_to_text API call.\n",
    "        # It's an agentic model.\n",
    "\n",
    "        # For Magma specifically, the API might be structured more like this (if it has a public inference API):\n",
    "        # endpoint = \"https://<your-magma-inference-endpoint>\"\n",
    "        # headers = {\"Authorization\": f\"Bearer {HF_TOKEN}\"}\n",
    "        # files = {\"image\": open(image_path, \"rb\")}\n",
    "        # data = {\"text\": text_input}\n",
    "        # response = requests.post(endpoint, headers=headers, files=files, data=data)\n",
    "        # return response.json()\n",
    "        return {\"error\": \"Could not process Magma request. Check model card for API specifics.\"}\n",
    "\n",
    "\n",
    "# Example of how you might call it from your UI backend\n",
    "# text_result = query_magma_text(\"What is the capital of France?\")\n",
    "# print(text_result)\n",
    "\n",
    "# image_text_result = query_magma_image_and_text(\"path/to/your/image.jpg\", \"What is visible in this image?\")\n",
    "# print(image_text_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "471b9914",
   "metadata": {},
   "outputs": [
    {
     "ename": "HTTPError",
     "evalue": "404 Client Error: Not Found for url: https://api-inference.huggingface.co/models/microsoft/Magma-8B",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mHTTPError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m text_result = \u001b[43mquery_magma_text\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mWhat is the capital of France?\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 26\u001b[39m, in \u001b[36mquery_magma_text\u001b[39m\u001b[34m(text_input)\u001b[39m\n\u001b[32m     18\u001b[39m payload = {\n\u001b[32m     19\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33minputs\u001b[39m\u001b[33m\"\u001b[39m: text_input\n\u001b[32m     20\u001b[39m     \u001b[38;5;66;03m# Magma often expects a specific prompt format, e.g., for VQA or UI navigation\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     23\u001b[39m     \u001b[38;5;66;03m# or for UI navigation, it might involve image data directly.\u001b[39;00m\n\u001b[32m     24\u001b[39m }\n\u001b[32m     25\u001b[39m response = requests.post(API_URL, headers=headers, json=payload)\n\u001b[32m---> \u001b[39m\u001b[32m26\u001b[39m \u001b[43mresponse\u001b[49m\u001b[43m.\u001b[49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# Raise an exception for bad status codes\u001b[39;00m\n\u001b[32m     27\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m response.json()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\rober\\Documents\\iminabo_stuff\\code\\my_Omni_Parser\\no_ai\\venv311\\Lib\\site-packages\\requests\\models.py:1026\u001b[39m, in \u001b[36mResponse.raise_for_status\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1021\u001b[39m     http_error_msg = (\n\u001b[32m   1022\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.status_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m Server Error: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mreason\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m for url: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.url\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m   1023\u001b[39m     )\n\u001b[32m   1025\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response=\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[31mHTTPError\u001b[39m: 404 Client Error: Not Found for url: https://api-inference.huggingface.co/models/microsoft/Magma-8B"
     ]
    }
   ],
   "source": [
    "text_result = query_magma_text(\"What is the capital of France?\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58d80da7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
